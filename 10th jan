import json
# import argparse
from pathlib import Path
import pandas as pd
from flask import Flask, request, jsonify
from flask_cors import CORS
import sqlite3
from unsloth import FastLanguageModel
from utils.region_locator import UserPS

app = Flask(__name__)

CORS(app, origins="*", allow_headers='Content-Type', allow_methods=['GET', 'POST'])

userps = UserPS()

max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.
alpaca_prompt = """Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Response:
"""

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "llama3.1_sql-python",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)
FastLanguageModel.for_inference(model)

def user_ps(location):
    # print('location:', location)
    ps_name = userps.location(location)
    return ps_name

def excel_to_sqlite3(df:pd, conn:sqlite3):
  ps_info = df.drop(['PS_Latitude', 'PS_Longitude'], axis=1)
  ps_loc = df[['PS_Name', 'PS_Latitude', 'PS_Longitude']]
  ps_info.to_sql("PS_Info", conn, if_exists='replace', index=False)
  ps_loc.to_sql("PS_Loc", conn, if_exists='replace', index=False)
  return "data inserted to sqlite3"

def get_cursor():
  if not Path('data/police_data.db').exists():
    file_path = "data/police_bot.xlsx"
    df = pd.read_excel(file_path)
    conn = sqlite3.connect("data/police_data.db")
    excel_to_sqlite3(df, conn)
  else:
    conn = sqlite3.connect('data/police_data.db')
  return conn, conn.cursor()

def query_sqlite3(query:str):
  conn, cursor = get_cursor()
  cursor.execute(query,)
  rows = cursor.fetchall()
  column_names = [descrp[0] for descrp in cursor.description]
  if len(rows) > 0:
    result = {k:v for k, v in zip(column_names, rows[0])}
  else:
    result = {"Casual": 'No information found'}  
  result_json = json.dumps(result)
  conn.close()
  return result_json

def query_sql(query):
    inputs = tokenizer(
    [
        alpaca_prompt.format(
            instruction=query, # instruction
            # output - leave this blank for generation!
        )
    ], return_tensors = "pt").to("cuda")
    outputs = model.generate(**inputs, max_new_tokens = 256)
    response = tokenizer.batch_decode(
        outputs, skip_special_tokens=True, 
        clean_up_tokenization_spaces=True)[0].split('Response:\n')[-1].replace("'s", "")
    return response

def warm_up():
  warm_up_questions = ['hi', 'Hi', 'who are you?',
                       'who are you', 'what can you do?',
                       'who developed you?']
  for qus in warm_up_questions:
    query_sql(qus)
  print('warm up done')

@app.route('/query', methods=['POST'])
def main():
  # query from ui with person location coordinates
  json_data = request.get_json()
  query = json_data.get('query', '')
  coordinates = json_data.get('coordinates', '')
  # coordinates = (17.4366725,78.3643739)
  if not query:
    return jsonify({'Casual': 'How can I assist you?'})
  sql_response_str = query_sql(query)
  sql_response_updated = sql_response_str.replace(
      '{my_location}', 'my_location').replace('my_location', user_ps(coordinates))
  sql_response = eval(sql_response_updated)
  if sql_response['SQL']:
    final_response = query_sqlite3(sql_response['SQL'])
  elif sql_response['Func']:
    final_response = query_sqlite3(sql_response['Func'])
  else:
    final_response = {'Casual': sql_response['Casual']}
  return final_response

if __name__ == '__main__':
  warm_up()
  app.run(host='0.0.0.0', port=5000, debug=False)



























---------------------------------------------------------------------------------------------------------------------------------------------------------------------


import pandas as pd
from rapidfuzz import fuzz
import re
import logging

# Configure logger
logging.basicConfig(
    filename="matching_process.log",  # Log file
    filemode="w",  # Overwrite file on each run
    format="%(asctime)s - %(levelname)s - %(message)s",  # Log format
    level=logging.DEBUG,  # Log level
)

# Log the start of the process
logging.info("Starting the matching process...")

# Load Excel files
try:
    source_df = pd.read_excel(r"C:\\Users\\tejas\\Source.xlsx")
    destination_df = pd.read_excel(r"C:\\Users\\tejas\\Destination.xlsx")
    synonyms_df = pd.read_excel(r"C:\\Users\\tejas\\WlpStdNames.xlsx")
    mapping_df = pd.read_excel(r"C:\\Users\\tejas\\elv_mapping.xlsx")
    logging.info("Excel files loaded successfully.")
except Exception as e:
    logging.error(f"Error loading Excel files: {e}")
    raise e

# Step 1: Data Preparation
def normalize_string(s):
    """Normalize the string by removing special characters, converting to lowercase, and trimming."""
    if pd.isna(s):
        return ""
    s = re.sub(r'\W', ' ', str(s)).strip().lower()
    logging.debug(f"Normalized string: '{s}'")
    return s

def remove_duplicates(df, subset):
    """Remove duplicate rows based on specified columns."""
    before = len(df)
    df = df.drop_duplicates(subset=subset)
    after = len(df)
    logging.info(f"Removed {before - after} duplicates from dataframe.")
    return df

logging.info("Removing duplicates from source and destination files...")
source_df = remove_duplicates(source_df, ['Physical Column Description', 'Logical Column Name'])
destination_df = remove_duplicates(destination_df, ['Column Name', 'Attribute Description'])

logging.info("Normalizing strings in source and destination files...")
source_df['Logical Column Name'] = source_df['Logical Column Name'].apply(normalize_string)
destination_df['Attribute Description'] = destination_df['Attribute Description'].apply(normalize_string)
destination_df['Column Name'] = destination_df['Column Name'].apply(normalize_string)

# Step 2: Apply Synonyms and Abbreviations
def apply_mappings(text, synonyms_df, mapping_df):
    """Apply synonyms and mappings to standardize column names."""
    logging.debug(f"Original text for mapping: '{text}'")
    for _, row in synonyms_df.iterrows():
        text = re.sub(fr'\b{normalize_string(row["col_name"])}\b', normalize_string(row["col_short_name"]), text)
    for _, row in mapping_df.iterrows():
        text = re.sub(fr'\b{normalize_string(row["col_name"])}\b', normalize_string(row["elv_mapping"]), text)
    logging.debug(f"Text after applying mappings: '{text}'")
    return text

logging.info("Applying synonyms and abbreviations...")
source_df['Logical Column Name'] = source_df['Logical Column Name'].apply(lambda x: apply_mappings(x, synonyms_df, mapping_df))
destination_df['Attribute Description'] = destination_df['Attribute Description'].apply(lambda x: apply_mappings(x, synonyms_df, mapping_df))

# Step 3: Extract Prefix and Number
def extract_prefix_number(text):
    """Extract prefix and number from a string."""
    match = re.search(r'(.*?)[ _-]?(\d+)[a-z_]*$', text)
    if match:
        prefix, number = match.groups()
        logging.debug(f"Extracted prefix: '{prefix}', number: '{number}' from text: '{text}'")
        return prefix.strip(), number
    logging.debug(f"No number found in text: '{text}'")
    return text.strip(), None

logging.info("Extracting prefixes and numbers from source file...")
source_df['Prefix'], source_df['Number'] = zip(*source_df['Logical Column Name'].apply(extract_prefix_number))

# Step 4: Fuzzy Matching
def find_best_match(source, destination):
    """Find the best match for a source value in the destination list using fuzzy matching."""
    best_match, best_score = None, 0
    for dest in destination:
        score = fuzz.partial_ratio(source, dest)
        logging.debug(f"Fuzzy matching '{source}' with '{dest}' - Score: {score}")
        if score > best_score:
            best_match, best_score = dest, score
    logging.info(f"Best match for '{source}' is '{best_match}' with score: {best_score}")
    return best_match, best_score

logging.info("Performing fuzzy matching...")
results = []
destination_columns = destination_df['Attribute Description'].tolist() + destination_df['Column Name'].tolist()

for _, row in source_df.iterrows():
    logical_name = row['Logical Column Name']
    prefix, number = row['Prefix'], row['Number']
    logging.info(f"Processing Logical Column Name: '{logical_name}'")

    # Match using number
    number_matches = [col for col in destination_columns if number and number in col]
    logging.debug(f"Number matches for '{number}': {number_matches}")

    # If number matches found, refine by prefix
    if number_matches:
        best_match, best_score = find_best_match(prefix, number_matches)
    else:
        best_match, best_score = find_best_match(logical_name, destination_columns)

    results.append({
        "Logical Column Name": logical_name,
        "Best Match": best_match,
        "Score": best_score
    })

# Convert results to DataFrame
results_df = pd.DataFrame(results)

#_________________________________________________________________________________________________________________________________________________________________________________________________________________

import pandas as pd
from rapidfuzz import process, fuzz
from difflib import SequenceMatcher
import re
import logging

# Setup logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# Load Excel files
source_df = pd.read_excel(r"C:\Users\tejas\Source.xlsx")
destination_df = pd.read_excel(r"C:\Users\tejas\Destination.xlsx")
synonyms_df = pd.read_excel(r"C:\Users\tejas\WlpStdNames.xlsx")
mapping_df = pd.read_excel(r"C:\Users\tejas\elv_mapping.xlsx")

# Helper functions
def remove_duplicates(df, subset):
    logging.debug(f"Removing duplicates based on columns: {subset}")
    return df.drop_duplicates(subset=subset)

# Normalize and clean strings
def normalize_string(s):
    s = re.sub(r'\W+', ' ', s).strip().lower()
    logging.debug(f"Normalized string: {s}")
    return s

# Apply synonym and abbreviation mapping
def apply_mappings(text):
    original_text = text
    for _, row in synonyms_df.iterrows():
        text = re.sub(fr'\b{row["col_name"].lower()}\b', row["col_short_name"].lower(), text)
    for _, row in mapping_df.iterrows():
        text = re.sub(fr'\b{row["col_name"].lower()}\b', row["elv_mapping"].lower(), text)
    logging.debug(f"Applied mappings: Original text = {original_text}, Mapped text = {text}")
    return text

# String matching using multiple scorers
def string_match(a, b):
    scores = [
        SequenceMatcher(None, a, b).ratio(),
        fuzz.ratio(a, b) / 100,
        fuzz.partial_ratio(a, b) / 100,
        fuzz.token_set_ratio(a, b) / 100,
        fuzz.token_sort_ratio(a, b) / 100
    ]
    max_score = max(scores)
    logging.debug(f"String match scores: {scores}, Max score = {max_score}")
    return max_score

def find_top_matches(source_values, final_destination_df, limit=5, threshold=40):
    matches = []
    unmatched_values = []
    
    destination_attr_values = final_destination_df['Attribute Name'].astype(str).apply(normalize_string).apply(apply_mappings).tolist()
    destination_column_values = final_destination_df['Column Name'].astype(str).apply(normalize_string).apply(apply_mappings).tolist()
    destination_column_names = final_destination_df['Column Name'].astype(str).tolist()
    
    logging.debug("Starting fuzzy matching process...")
    for source_val in source_values:
        normalized_source_val = apply_mappings(normalize_string(source_val))
        logging.debug(f"Processing source value: {source_val}, Normalized value: {normalized_source_val}")
        
        # Combine attribute and column values for matching
        combined_destinations = destination_attr_values + destination_column_values
        
        # Perform fuzzy matching
        top_matches = process.extract(normalized_source_val, combined_destinations, scorer=fuzz.token_sort_ratio, limit=limit)
        logging.debug(f"Top matches for '{source_val}': {top_matches}")
        
        # Filter matches based on threshold
        filtered_matches = [match for match in top_matches if match[1] >= threshold]
        
        # Store matches or mark as unmatched
        if filtered_matches:
            for match in filtered_matches[:5]:
                idx = combined_destinations.index(match[0])
                column_name = destination_column_names[idx % len(destination_column_names)] if idx < len(destination_column_names) else None
                matches.append({
                    'Source Value': source_val,
                    'Matched Value': match[0],
                    'Matched Column Name': column_name,
                    'Score': round(match[1])
                })
        else:
            unmatched_values.append(source_val)
            logging.debug(f"No match found for source value: {source_val}")

    return matches, unmatched_values

# Remove duplicates and prepare data
final_source_df = remove_duplicates(source_df, subset=['Physical Column Description', 'Logical Column Name'])
final_destination_df = remove_duplicates(destination_df, subset=['Column Name', 'Attribute Description'])

source_values = final_source_df['Logical Column Name'].astype(str).tolist()
threshold_value = 40

# Match results
matched_results, unmatched_results = find_top_matches(source_values, final_destination_df, limit=5, threshold=threshold_value)
matched_df = pd.DataFrame(matched_results)

# Log final results
logging.info(f"Matched results:\n{matched_df}")
logging.info(f"Unmatched values:\n{unmatched_results}")
