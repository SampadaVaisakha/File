import pandas as pd
from rapidfuzz import fuzz
import re

# Load Excel files
source_df = pd.read_excel(r"C:\Users\tejas\Source.xlsx")
destination_df = pd.read_excel(r"C:\Users\tejas\Destination.xlsx")
synonyms_df = pd.read_excel(r"C:\Users\tejas\WlpStdNames.xlsx")
mapping_df = pd.read_excel(r"C:\Users\tejas\elv_mapping.xlsx")

# Normalize strings (remove special characters and lowercase)
def normalize_string(s):
    return re.sub(r'\W+', ' ', str(s)).strip().lower()

# Apply mappings to replace shortcuts and synonyms
def apply_mappings(text):
    for _, row in synonyms_df.iterrows():
        text = re.sub(fr'\b{row["col_name"].lower()}\b', row["col_short_name"].lower(), text)
    for _, row in mapping_df.iterrows():
        text = re.sub(fr'\b{row["col_name"].lower()}\b', row["elv_mapping"].lower(), text)
    return text

# Extract tokens for fuzzy matching
def extract_tokens(text):
    return re.findall(r'\w+', text.lower())

# Match logic with handling of NaN in destination list
def custom_match(source_val, destination_df):
    best_match = None
    highest_score = 0
    source_tokens = extract_tokens(source_val)

    for _, row in destination_df.iterrows():
        dest_val = row['Normalized']
        dest_column = row['Column Name']

        # If 'Attribute Name' is NaN, fall back to 'Column Name'
        if pd.isna(dest_val):
            dest_val = dest_column
        
        dest_tokens = extract_tokens(dest_val)

        # Token overlap score
        token_overlap = len(set(source_tokens) & set(dest_tokens)) / len(set(source_tokens))
        score = fuzz.partial_ratio(source_val, dest_val) * 0.7 + token_overlap * 30

        # Update best match if score is higher
        if score > highest_score:
            highest_score = score
            best_match = {
                'Matched Attribute': dest_val,
                'Matched Column Name': dest_column,
                'Score': round(score)
            }

    return best_match

# Match source values with destination values
def find_top_matches(source_values, destination_df, threshold=70):
    matches = []
    unmatched = []

    # Normalize and map destination values
    destination_df['Normalized'] = destination_df['Attribute Name'].astype(str).apply(normalize_string).apply(apply_mappings)

    for source_val in source_values:
        normalized_source_val = apply_mappings(normalize_string(source_val))

        best_match = custom_match(normalized_source_val, destination_df)

        if best_match and best_match['Score'] >= threshold:
            matches.append({
                'Source Value': source_val,
                **best_match
            })
        else:
            unmatched.append(source_val)

    return matches, unmatched

# Remove duplicates from source and destination files
def remove_duplicates(df, subset):
    return df.drop_duplicates(subset=subset)

final_source_df = remove_duplicates(source_df, subset=['Physical Column Description', 'Logical Column Name'])
final_destination_df = remove_duplicates(destination_df, subset=['Column Name', 'Attribute Description'])

# Get source values for matching
source_values = final_source_df['Logical Column Name'].astype(str).tolist()

# Perform matching
matched_results, unmatched_results = find_top_matches(source_values, final_destination_df)

# Create DataFrame for matches
matched_df = pd.DataFrame(matched_results)
matched_df.to_excel('matched_results.xlsx', index=False)

# Print unmatched values
print("Unmatched Values:", unmatched_results)
