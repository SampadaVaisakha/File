

import pandas as pd
from rapidfuzz import process, fuzz
from difflib import SequenceMatcher
import re

# Load Excel files
source_df = pd.read_excel(r"C:\Users\tejas\Source.xlsx")
destination_df = pd.read_excel(r"C:\Users\tejas\Destination.xlsx")
synonyms_df = pd.read_excel(r"C:\Users\tejas\WlpStdNames.xlsx")
mapping_df = pd.read_excel(r"C:\Users\tejas\elv_mapping.xlsx")

# Helper functions
def remove_duplicates(df, subset):
    return df.drop_duplicates(subset=subset)

# Normalize and clean strings
def normalize_string(s):
    s = re.sub(r'\W+', ' ', s).strip().lower()
    return s

# Apply synonym and abbreviation mapping
def apply_mappings(text):
    for _, row in synonyms_df.iterrows():
        text = re.sub(fr'\b{row["col_name"].lower()}\b', row["col_short_name"].lower(), text)
    for _, row in mapping_df.iterrows():
        text = re.sub(fr'\b{row["col_name"].lower()}\b', row["elv_mapping"].lower(), text)
    return text

# String matching using multiple scorers
def string_match(a, b):
    return max(
        SequenceMatcher(None, a, b).ratio(),
        fuzz.ratio(a, b) / 100,
        fuzz.partial_ratio(a, b) / 100,
        fuzz.token_set_ratio(a, b) / 100,
        fuzz.token_sort_ratio(a, b) / 100
    )

# Find top matches between source and destination data
def find_top_matches(source_values, final_destination_df, limit=5, threshold=40):
    matches = []
    unmatched_values = []
    
    destination_attr_values = final_destination_df['Attribute Name'].astype(str).apply(normalize_string).apply(apply_mappings).tolist()
    destination_column_names = final_destination_df['Column Name'].astype(str).tolist()
    
    for source_val in source_values:
        normalized_source_val = apply_mappings(normalize_string(source_val))
        top_matches = process.extract(normalized_source_val, destination_attr_values, scorer=fuzz.token_sort_ratio, limit=limit)
        top_matches = sorted(set(top_matches), key=lambda x: x[1], reverse=True)
        
        filtered_matches = [match for match in top_matches if match[1] >= threshold]
        
        if not filtered_matches:
            for attr_val, col_name in zip(destination_attr_values, destination_column_names):
                score = string_match(normalized_source_val, attr_val)
                if score * 100 >= threshold:
                    filtered_matches.append((attr_val, score * 100, col_name))

        if filtered_matches:
            for match in filtered_matches[:5]:
                idx = destination_attr_values.index(match[0]) if match[0] in destination_attr_values else None
                matches.append({
                    'Source Value': source_val,
                    'Matched Attribute': match[0],
                    'Matched Column Name': destination_column_names[idx],
                    'Score': round(match[1])
                })
        else:
            unmatched_values.append(source_val)

    return matches, unmatched_values

# Remove duplicates and prepare data
final_source_df = remove_duplicates(source_df, subset=['Physical Column Description', 'Logical Column Name'])
final_destination_df = remove_duplicates(destination_df, subset=['Column Name', 'Attribute Description'])

source_values = final_source_df['Logical Column Name'].astype(str).tolist()
threshold_value = 40

# Match results
matched_results, unmatched_results = find_top_matches(source_values, final_destination_df, limit=5, threshold=threshold_value)
matched_df = pd.DataFrame(matched_results)
#_______________________________________________________________________________________________________________________________________________________________________________



import pandas as pd
from rapidfuzz import process, fuzz
from difflib import SequenceMatcher
import re

# Load Excel files
source_df = pd.read_excel(r"C:\Users\tejas\Source.xlsx")
destination_df = pd.read_excel(r"C:\Users\tejas\Destination.xlsx")
synonyms_df = pd.read_excel(r"C:\Users\tejas\WlpStdNames.xlsx")
mapping_df = pd.read_excel(r"C:\Users\tejas\elv_mapping.xlsx")

# Helper functions
def remove_duplicates(df, subset):
    return df.drop_duplicates(subset=subset)

# Normalize and clean strings
def normalize_string(s):
    s = re.sub(r'\W+', ' ', s).strip().lower()
    return s

# Apply synonym and abbreviation mapping
def apply_mappings(text):
    for _, row in synonyms_df.iterrows():
        text = re.sub(fr'\b{row["col_name"].lower()}\b', row["col_short_name"].lower(), text)
    for _, row in mapping_df.iterrows():
        text = re.sub(fr'\b{row["col_name"].lower()}\b', row["elv_mapping"].lower(), text)
    return text

# String matching using multiple scorers
def string_match(a, b):
    return max(
        SequenceMatcher(None, a, b).ratio(),
        fuzz.ratio(a, b) / 100,
        fuzz.partial_ratio(a, b) / 100,
        fuzz.token_set_ratio(a, b) / 100,
        fuzz.token_sort_ratio(a, b) / 100
    )

# # Find top matches between source and destination data
# def find_top_matches(source_values, final_destination_df, limit=5, threshold=40):
#     matches = []
#     unmatched_values = []
    
#     destination_attr_values = final_destination_df['Attribute Name'].astype(str).apply(normalize_string).apply(apply_mappings).tolist()
#     destination_column_names = final_destination_df['Column Name'].astype(str).tolist()
    
#     for source_val in source_values:
#         normalized_source_val = apply_mappings(normalize_string(source_val))
#         top_matches = process.extract(normalized_source_val, destination_attr_values, scorer=fuzz.token_sort_ratio, limit=limit)
#         top_matches = sorted(set(top_matches), key=lambda x: x[1], reverse=True)
        
#         # Direct exact match for DIAGNOSIS TYPECODE handling
#         source_match_number = re.search(r'typecode (\d+)$', normalized_source_val)
#         direct_match = []
#         if source_match_number:
#             match_number = source_match_number.group(1)
#             direct_match = [col for col in destination_column_names if re.fullmatch(rf'diag[_ ]?{match_number}[_ ]?cd', col.lower())]
        
#         if direct_match:
#             for col in direct_match:
#                 matches.append({
#                     'Source Value': source_val,
#                     'Matched Attribute': col,
#                     'Matched Column Name': col,
#                     'Score': 100
#                 })
#         else:
#             filtered_matches = [match for match in top_matches if match[1] >= threshold]
            
#             if not filtered_matches:
#                 for attr_val, col_name in zip(destination_attr_values, destination_column_names):
#                     score = string_match(normalized_source_val, attr_val)
#                     if score * 100 >= threshold:
#                         filtered_matches.append((attr_val, score * 100, col_name))

#             if filtered_matches:
#                 for match in filtered_matches[:5]:
#                     idx = destination_attr_values.index(match[0]) if match[0] in destination_attr_values else None
#                     matches.append({
#                         'Source Value': source_val,
#                         'Matched Attribute': match[0],
#                         'Matched Column Name': destination_column_names[idx],
#                         'Score': round(match[1])
#                     })
#             else:
#                 unmatched_values.append(source_val)

#     return matches, unmatched_values



def extract_prefix_number(text):
    match = re.search(r'(.*?)(\d+)$', text.lower())
    if match:
        prefix, number = match.groups()
        return prefix.strip(), number
    return text, None


def find_top_matches(source_values, final_destination_df, limit=5, threshold=40):
    matches = []
    unmatched_values = []
    
    destination_attr_values = final_destination_df['Attribute Name'].astype(str).apply(normalize_string).apply(apply_mappings).tolist()
    destination_column_names = final_destination_df['Column Name'].astype(str).tolist()
    
    for source_val in source_values:
        normalized_source_val = apply_mappings(normalize_string(source_val))
        source_prefix, source_number = extract_prefix_number(source_val)
        
        # Perform fuzzy matching first
        top_matches = process.extract(normalized_source_val, destination_attr_values, scorer=fuzz.token_sort_ratio, limit=limit)
        
        # Strict prefix + number match
        if source_number:
            filtered_matches = [
                match for match in top_matches
                if re.search(rf'{re.escape(source_prefix)}[\s_]*{source_number}', match[0].lower()) and match[1] >= threshold
            ]
        else:
            filtered_matches = [match for match in top_matches if match[1] >= threshold]
        
        # Prioritize exact number match
        if not filtered_matches and source_number:
            filtered_matches = [
                match for match in top_matches
                if re.search(rf'{source_number}\b', match[0].lower()) and match[1] >= threshold
            ]
        
        # Store matches or mark as unmatched
        if filtered_matches:
            for match in filtered_matches[:5]:
                idx = destination_attr_values.index(match[0]) if match[0] in destination_attr_values else None
                matches.append({
                    'Source Value': source_val,
                    'Matched Attribute': match[0],
                    'Matched Column Name': destination_column_names[idx],
                    'Score': round(match[1])
                })
        else:
            unmatched_values.append(source_val)

    return matches, unmatched_values


# Remove duplicates and prepare data
final_source_df = remove_duplicates(source_df, subset=['Physical Column Description', 'Logical Column Name'])
final_destination_df = remove_duplicates(destination_df, subset=['Column Name', 'Attribute Description'])

source_values = final_source_df['Logical Column Name'].astype(str).tolist()
threshold_value = 40

# Match results
matched_results, unmatched_results = find_top_matches(source_values, final_destination_df, limit=5, threshold=threshold_value)
matched_df = pd.DataFrame(matched_results)


