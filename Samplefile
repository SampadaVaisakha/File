import pandas as pd
import re
from concurrent.futures import ThreadPoolExecutor

# Load Excel files
source_df = pd.read_excel(r"C:\Users\tejas\Source.xlsx")
destination_df = pd.read_excel(r"C:\Users\tejas\Destination.xlsx")
synonyms_df = pd.read_excel(r"C:\Users\tejas\WlpStdNames.xlsx")
mapping_df = pd.read_excel(r"C:\Users\tejas\elv_mapping.xlsx")

# Normalize and clean strings
def normalize_string(s):
    return re.sub(r'\W+', ' ', str(s)).strip().lower()

# Create dictionaries for mappings
def build_mapping_dict(df, col_from, col_to):
    mapping = {}
    for _, row in df.iterrows():
        mapping[normalize_string(row[col_from])] = normalize_string(row[col_to])
    return mapping

synonym_dict = build_mapping_dict(synonyms_df, "col_name", "col_short_name")
mapping_dict = build_mapping_dict(mapping_df, "col_name", "elv_mapping")

# Apply synonym and abbreviation mappings
def apply_mappings(text):
    normalized_text = normalize_string(text)
    words = normalized_text.split()
    mapped_words = [mapping_dict.get(word, synonym_dict.get(word, word)) for word in words]
    return " ".join(mapped_words)

# Extract prefix and number
def extract_prefix_number(text):
    match = re.search(r'(.*?)(\d+)$', text.lower())
    if match:
        prefix, number = match.groups()
        return prefix.strip(), number
    return text, None

# Jaccard similarity for token overlap
def jaccard_similarity(a, b):
    a_tokens = set(a.split())
    b_tokens = set(b.split())
    intersection = len(a_tokens & b_tokens)
    union = len(a_tokens | b_tokens)
    return intersection / union if union != 0 else 0

# Token similarity (for handling reordered terms)
def token_similarity(a, b):
    a_tokens = set(re.findall(r'\w+', a))
    b_tokens = set(re.findall(r'\w+', b))
    return len(a_tokens & b_tokens) / max(len(a_tokens), len(b_tokens)) if a_tokens and b_tokens else 0

# Matching logic
def match_source_to_destination(source_val, destination_list, destination_columns, threshold=0.75):
    normalized_source_val = apply_mappings(source_val)
    source_prefix, source_number = extract_prefix_number(normalized_source_val)
    
    best_match = None
    best_score = 0
    
    for idx, dest_val in enumerate(destination_list):
        dest_prefix, dest_number = extract_prefix_number(dest_val)
        
        # Strict prefix and number matching
        if source_number and source_number == dest_number and source_prefix in dest_prefix:
            return {
                'Source Value': source_val,
                'Matched Attribute': dest_val,
                'Matched Column Name': destination_columns[idx],
                'Score': 100
            }
        
        # Compute Jaccard similarity and token similarity
        jaccard_score = jaccard_similarity(normalized_source_val, dest_val)
        token_score = token_similarity(normalized_source_val, dest_val)
        
        # Weighted scoring
        combined_score = (jaccard_score * 0.5) + (token_score * 0.5)
        
        if combined_score > best_score:
            best_score = combined_score
            best_match = dest_val

    # Return match if score meets threshold
    if best_score >= threshold:
        idx = destination_list.index(best_match)
        return {
            'Source Value': source_val,
            'Matched Attribute': best_match,
            'Matched Column Name': destination_columns[idx],
            'Score': round(best_score * 100)
        }
    return None

# Main matching function with parallel processing
def find_top_matches_parallel(source_values, destination_df, threshold=0.75):
    destination_df['Normalized'] = destination_df['Attribute Column'].astype(str).apply(normalize_string).apply(apply_mappings)
    destination_list = destination_df['Normalized'].tolist()
    destination_columns = destination_df['Column Name'].tolist()
    
    matches = []
    unmatched = []

    # Use ThreadPoolExecutor for parallel processing
    with ThreadPoolExecutor() as executor:
        futures = [
            executor.submit(match_source_to_destination, source_val, destination_list, destination_columns, threshold)
            for source_val in source_values
        ]
        for future in futures:
            result = future.result()
            if result:
                matches.append(result)
            else:
                unmatched.append(future.args[0])
    
    return matches, unmatched

# Preprocess and match
final_source_df = source_df.drop_duplicates(subset=['Logical Column Name'])
final_destination_df = destination_df.drop_duplicates(subset=['Column Name'])

source_values = final_source_df['Logical Column Name'].astype(str).tolist()

matched_results, unmatched_results = find_top_matches_parallel(source_values, final_destination_df)

# Save results
matched_df = pd.DataFrame(matched_results)
matched_df.to_excel('matched_results_optimized.xlsx', index=False)

print("Unmatched Values:", unmatched_results)
